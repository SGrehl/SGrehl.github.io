---
  title: "Experimental Sociology - Week 02"
  subtitle: "Analytical Methods: Decision & Game Theory"
  bibliography: references.bib
---

In this chapter, we'll delve into the formal **representation** and **analysis** of social interactions through **decision theory** and **game theory**. Our aim is to provide you with the fundamental tools for analytically modeling a variety of social phenomena.

**Why decision theory and game theory?** First, these approaches align with **methodological individualism** in that they focus on the micro-level situations individuals face and use those insights to generate predictions at both the micro and macro levels. Second, these frameworks provide structured, systematic methods to **model social situations**, making it easier to navigate complex interactions. Finally, they also offer robust techniques for **analyzing these models** and deriving meaningful hypotheses. Nevertheless, while decision and game theory provide relatively simple, they are not the only valid approaches. Numerous modifications [e.g., prospect theory, @tversky1981, @tversky1992] and alternative frameworks [e.g., bounded rationality, @rubinstein1998] are readily available, and they should be employed whenever you believe the additional insights justify the added complexity.

::: callout-tip
This chapter is an **introduction**, rather than a comprehensive course in decision or game theory. If youâ€™re already familiar with these concepts, the concise explanations in the info boxes might suffice as a refresher. For a more in-depth exploration, we recommend specialized resources such as @gilboa2010 for an accessible overview or @osborne2004 for a more rigorous, formal perspective.
:::

## ðŸŽ¯ Learning Goals

By the end of this chapter, you should:

-   **Model** **simple social situations** using decision theory or game theory.
-   **Analyze these situations** using appropriate analytical tools from **decision theory** or **game theory**.

# Modelling Social Interactions

Central to this process is understanding behaviorâ€”peopleâ€™s actions have a direct bearing on how we address societal challenges. For example, consider the question, â€œHow can we reduce discriminatory behavior?â€ Here, itâ€™s clear that behavior itself is crucial: discrimination is enacted through observable actions. Yet, to combat discrimination effectively, researchers also investigate underlying attitudes (like prejudices or biases) and the cultural products (books, movies, music) that might indirectly influence discrimination.

In many cases, weâ€™re interested in a core outcomeâ€”such as shifting attitudes or assessing whether certain initiatives succeed or fail. However, these outcomes typically hinge on actual behavior: how people speak, what they consume, and how they respond to one another. Even when our primary focus isnâ€™t directly on behavior, the factors shaping or driving that outcome often are forms of behavior themselves. Recognizing this central role of behavior is essential for fully understanding and tackling social problems.

Decision theory and game theory emphasize the centrality of behavior but apply differently depending on the social context. **Decision theory** addresses situations where the outcome depends solely on the decisions of an **individual actor**, without direct interference from others. For example, choosing whether to pursue higher education or immediately enter the labor market is primarily a personal decision that can be analyzed using decision theory. These situations are formally termed parametric ***decisions*** because the model predictions primarily depend on situational parameters.

In contrast, **game theory** applies to scenarios where outcomes are influenced by the interactions of **multiple individuals** or groups, whose **decisions are interdependent**. This means no single actor has complete control over the outcome, adding a strategic dimension to their decision-making. Examples include negotiations, competitive markets, or volunteering for a task. These situations are formally termed ***games*** due to their interactive and strategic nature.[^1]

[^1]: Decision theory is sometimes referred to as *parametric decision theory*. Game theory, on the other hand, could actually be called *strategic decision theory*, since outcomes depend significantly on strategic considerations and the choices of others. Despite this, game theory retains its playful name from its origins in analyzing parlor games [@neumann1944]. A fitting testament to how tradition (and a dash of path dependence) can leave a lasting imprint on terminology.

Occasionally, categorizing a situation clearly as either a decision or a game may not be straightforward. Take, for instance, the choice of whether to attend university or join the workforce immediately. Initially, this might appear to be an individual decision. However, parental approval or financial support can significantly influence the outcome, suggesting elements of strategic interaction. As social scientists, we have the flexibility to model such scenarios either as decisions or as games, depending on which perspective best captures the dynamics involved. Making this choice entails analytical responsibility, as it shapes how we understand, analyze, and interpret the social situation. Later, we will examine an example that demonstrates how these contrasting approaches can yield markedly different predictions.

::: callout-note
In summary:

-   A **decision** involves a parametric choice of only **one actor** and is analyzed using **decision theory**.
-   A **game** involves strategic interactions among **two or more actors** and is analyzed using **game theory**.
:::

## Behavior vs. Action

Before proceeding further, let's clarify key concepts: **behavior** and **action**. Although these terms often appear interchangeable in everyday language, in social science they carry distinct meanings.

In many fields of social science, behavior is used broadly to describe any observable conduct that originates with the individual, while action typically refers to behavior resulting from a deliberate, goal-directed decision process [@elster2007,163f]. Non-intentional or automatic conductâ€”such as reflexesâ€”would thus be excluded from action. However, it can be challenging to differentiate reflexive, automatic behavior from genuinely voluntary action in practice [@selten2002, @sumner2008]. For these reasons, we will not adopt this strict distinction based on intentionality.

From an **empirical standpoint**, there is another point to consider: we often cannot observe an actorâ€™s true intentions, only the outward behavior they display. For instance, imagine two sumo wrestlers in the same match. One might genuinely be trying to win, while the other might only appear to be making an effort but is secretly aiming to loseâ€”perhaps due to gambling incentives or other pressures. Despite these different **underlying actions**, the **observable behavior** of both wrestlers may look nearly identical. As researchers, we typically see only the outcome (who won or lost) and perhaps some surface cues during the match; the wrestlerâ€™s concealed motivations or private calculations remain hidden.

Nevertheless, even with these limitations, we can design models that distinguish between potential underlying actions. By analyzing patterns in outcomes, along with additional data [cf. @duggan2002], we may draw reasonable inferences about whether behaviors result from genuine intentions or ulterior motives.

::: callout-note
In this course, we will use the following terminology:

-   **Action** is what the actor deliberately aims to do. We often **cannot directly observe** this.
-   **Behavior** is the **observable action** that researchers see. Different actions can lead to the **same** observed behavior.

Note, however, that this approach differs from distinctions found in other sources [e.g. @anscombe2000, @elster2007].
:::

In many research contexts, however, we do not require a strict separation between these concepts; the broader category of observed behavior often suffices, unless we specifically need to focus on strategic or hidden intentions that could significantly alter our interpretation of outcomes.

# Decision Theory

Decision theory provides a foundational toolkit for modelling individual choices under certainty and uncertainty. Because game theory builds on many of the same principles, it's helpful to begin by establishing a solid grasp of decision theory. We'll outline the fundamental elements and constraints of decision-making before examining more complex, strategic interactions.

## Modelling

In order to model a decision, we need to know the fundamental components of decision theory. These are:

-   **Restrictions** (constraints and limitations influencing the range of possible actions),
-   **Preferences** (rankings or valuations of potential outcomes).
-   **Beliefs** (subjective probabilities and expectations about uncertain states of the world),

In the following sections, we will explore each of these components individually and illustrate their importance through relevant examples.

### Restrictions

Restrictions can be divided into two main categories. The first addresses the actorâ€™s own behavioral optionsâ€”what actions are actually feasible in a given situation? The second covers external factors that may influence the actor yet lie outside of their direct control. From the first perspective, we derive the **action set**, and from the second, the **set of all possible states of the world**. We will examine each of these in detail. Finally, combining the action set with the states of the world yields the possible **consequences**, which we will also explore.

### Action set

**Restrictions** regarding the actor ask: *What range of possible actions do actors have in a given decision-making scenario?* These limitations can arise from numerous sources, including the actorâ€™s resources, available information, and broader social or institutional constraints.

-   **Resources**: Money, time, effort, and technical know-how all help define the set of feasible actions. A studentâ€™s decision on where to study, for instance, can be constrained by scholarship opportunities, personal savings, or family responsibilities.
-   **Information and Knowledge**: Actors must have at least minimal awareness of their own possibilities, risks, and probable outcomes. A lack of informationâ€”like being unaware that a particular course of study existsâ€”can effectively remove that option from the feasible set.
-   **Social and Institutional Constraints**: Legal restrictions, cultural norms, or social sanctions can also limit the set of feasible actions. For example, certain forms of expression might be legally prohibited or socially stigmatized in a particular society.
-   **Psychological Constraints**: Beyond purely external obstacles, actors may also face cognitive or emotional barriers, such as fear of failure or lack of self-efficacy. These personal obstacles can prevent them from pursuing otherwise feasible options.

Formally, we collect these limitations into a **feasible action set** , representing all the courses of action an actor can realistically pursue. When modeling a situation, it is crucial to define carefully. Overlooking relevant restrictions can lead to unrealistic models and poor predictions of actual behavior.

Examples would be

$$A= \left\{ \text{Write term paper in English}, \text{Write term paper in German}, \text{Write term paper in Klingon} \right\}$$

or

$$A= \left\{ \text{Summer clothes}, \text{winter clothes}, \text{rain clothes} \right\}$$

Note that only **one** action can be chosen from the action set at a time. If an actor wants to wear rain clothes **and** summer clothes together, that combination must be defined as a **separate** option in the action set. In other words, actors cannot choose more than one action **simultaneously**.

::: callout-note
-   **One aspect of restrictions** refer to any factor that reduces an actorâ€™s possible range of actions; they are **indirectly accounted** for by defining the actorâ€™s **action set**.

-   The **action set** includes all **feasible actions** that the actor can take in the modeled social situation. Only one action can be chosen at a time.
:::

### State of the world

The second form of restriction asks: *What range of possible external factors can occur in the decision-making scenario?* A **state of the world**, denoted by $\omega$, is a concise description of all relevant external factors affecting a given situation and which the actor cannot control. Formally, we represent the set of all possible states $\omega$ as $\Omega$. For instance:

$$\Omega = \left\{ \text{sunny weather}, \text{rainy weather} \right\}$$ if weather is the key external factor or

$$\Omega = \left\{ \text{morning}, \text{noon} , \text{evening} \right\}$$ if the time of day matters.

Likewise, only **one** state of the world can occur at a time. If you want to account for multiple external conditionsâ€”such as â€œrainy morningâ€ versus â€œrainy eveningâ€â€”each of these **combined** states must appear **separately** in the set of possible states. The model does not allow more than one state of the world **simultaneously**.

If we have $|\Omega|=1$, meaning there is only a single possible state, this situation is referred to as **decision under certainty**, because the actor faces no uncertainty about external conditions. If $|\Omega|>1$, meaning there are at least two possible states, this situation is referred to as **decision under uncertainty**.

::: callout-note
-   A **state of the world** $\omega$ includes all external factors that are relevant for the modeled social situation.
:::

### Consequences

Once we specify both the set of possible actions $A$ an actor may take and the set of possible states of the world $\Omega$, each combination of an action $a \in A$ and a state $\omega \in \Omega$ yields a distinct **consequence**, $x$. Thus, we can define the set of all possible consequences as:

$$X=A \times \Omega$$This notation emphasizes that outcomes depend both on what the actor does (the chosen action) and on how external factors turn out (the actual state of the world). In @tbl-consequences illustrates how three actions and two states can combine to produce six potential **consequences**.

|                                 |                   |                   |
|---------------------------------|-------------------|-------------------|
| **actions states of the world** | **sunny weather** | **rainy weather** |
| **summer cloth**                | $x_{S,S}$         | $x_{S,R}$         |
| **winter cloth**                | $x_{W,S}$         | $x_{W,R}$         |
| **rain cloth**                  | $x_{R,S}$         | $x_{R,R}$         |

: Combining actions and states to form consequences. {#tbl-consequences}

::: callout-note
-   **Consequences** are the **combination of** an actors **actions** and a **state of the world**.
:::

### Preferences

In decision theory, **preferences** describe how an actor ranks or values potential outcomes (i.e. **consequences**). We typically use the notation $x \succsim y$ to indicate that outcome $x$ is **at least as preferred** as outcome $y$. This is called the **weak preference relation**. From it, we can derive two additional relations:

-   The **strict preference relation** ($\succ$): where $x \succ y$ means a strict preference for $x$ over $y$.

-   The **indifference relation** ($\sim$): where $x \sim y$ means that $x$ and $y$ are equally preferred.

Because these two relations follow from the weak preference relation ($x \succ y \Leftrightarrow x \succsim y \land \neg ( y \succsim x)$ and $x \sim y \Leftrightarrow x \succsim y \land y \succsim x$), we typically define only the weak preference relation explicitly. The strict preference and indifference relations are then **implicitly** included.

A **preference relation** is a form of [mathematical relations](https://en.wikipedia.org/wiki/Relation_(mathematics)) and must obey certain logical rules. For instance, it cannot simultaneously be true that $x \succsim y$ and $\neg (x \succsim y)$ hold at the same time (yet, it can be that $x \succsim y$ and $y \succsim x$ simultaneously hold, so make sure you understand the difference between these two notions). Beyond that, relations do not come with much rules. Yet, we may impose further **axioms** to capture how our imagined people (from which we later will derive a behavioral prediction) should react. For example, in many situations it seems quite sensible to model an actor how states $x \succ y$ and $y \succ z$, it should not be the case that $z \succ x$.[^2] Luckily, even a handful of such axioms can enable us to predict a wide range of behaviors.

[^2]: In addition, if we fail to rule out such cases, our model may produce no predictions at all, effectively rendering it uninformative. I personally prefer a model that risks being wrong over one that cannot generate any predictions, because an incorrect but testable model can always be refined and improved.

Moreover, having these axioms in place helps us **transform** an actorâ€™s preferences into **utility values** (see next section), making it easier to use mathematical and statistical tools for analysis.

#### Completeness, Transitivity, and Independence

As we will see when we come to analyze a social situation, two axioms are very important for the case of certainty. In case of uncertainty we also need a two addtional axioms:

1.  **Completeness**: For any two outcomes and , an actor can state whether is preferred to , is preferred to , or whether there is indifference between them . Formally, this means the actor never withholds a judgment between two outcomes.

2.  **Transitivity**: If and , then it must also be that . This requirement rules out cyclical or contradictory preferences. Without transitivity, consistent decision-making becomes problematic.

3.  **Continuity**: If $x\succsim y \succsim z$, then there exists a probability $p\in (0,1)$ such that $p\cdot x + (1-p) \cdot z \sim y$.[^3]

4.  **Independence of Irrelevant Alternatives** (**IIA**): If $x \succsim y$ holds, it must also hold $p \cdot x + (1-p) \cdot z \succsim  p \cdot y + (1-p) \cdot z$ for each $z$ and $p \in (0,1)$. That is the irrelevant alternative $z$ should not influence the existing preference $x \succsim y$ because it . This axiom is especially relevant for decisions under **risk** or **uncertainty**.

[^3]: Technically the "+" is somewhat troublesome in this context, since we cannot really "add" consequences $x$ and $z$ simply together. Basically we have here a lottery, where one gets with probability $p$ consequence $x$ and with $(1-p)$ the consequence $z$ (see lotteries). The same is true for the next axiom (IIA).

When preferences satisfy **completeness** and **transitivity**, we say the actor has an **ordinal preference ordering**, which is sufficient for consistent decision-making in situation under certainty. If we also assume the **continuity and IIA** axiom, we can achieve a **cardinal preference ordering**, allowing us to quantify not only the **rank** of preferences but also their **relative strength**. This lays the groundwork for using a expected utility function, which can capture preferences for a lottery of uncertain events.

### Utility

A **utility function** assigns numerical values to outcomes while preserving the actorâ€™s preference ranking. We say that a utility function $u$ **represents** a ordinal preference relation $\succsim$ if, for all $x,y \in X$ hold:

$$
x\succsim y \Leftrightarrow u(x) \geq u(y)
$$

In other words, whenever $x$ is at least as preferred as $y$, $u$ must assign to $x$ a value at least as large as the one it assigns to $y$. In @tbl-utility-ordinal we see that the ordinal preferences $x\succ y \succ z$ are represented by the utility function $u$ and $v$, but not by $h$ as $h(y)>h(x)$ contradicts with $x\succ y$.

| Assume $x\succ y \succ z$ | $x$ | $y$ | $z$   |
|---------------------------|-----|-----|-------|
| $u(\cdot)$                | 3   | 2   | 1     |
| $v(\cdot)$                | 42  | 11  | -3.14 |
| $h(\cdot)$                | 1   | 2   | 0     |

: Ordinal preferences and thus ordinal utility functions. While $u$ and $v$ represent the given preferences, $h$ does not. {#tbl-utility-ordinal}

With a utility function in hand, we have all the tools needed to analyze **decision situations under certainty**, where there is only one possible state of the world. However, if **uncertainty** exists (i.e., more than one possible state can occur), we need to introduce additional concepts before we can fully analyze those scenarios.

::: callout-note
-   A **utility function** assigns numerical values to any possible outcome. These numbers represent an actors preferences.
:::

### Expected Utility

When decisions involve **uncertainty**, that is, more than one possible state of the world can occur with varying likelihoods, we apply the concept of **expected utility**. This approach extends the idea of a **utility function** from deterministic outcomes (under certainty) to **probabilistic mixtures** of outcomes, commonly referred to as **lotteries**.

We represent a **lottery** as $[p_1,x_1;p_2,x_2]$. This means that if the lottery is chosen, the actor receives outcome $x_1$ with probability $p_1$, and outcome $x_2$ with probability $p_2$. Since probabilities must always add up to 1, in this case $p_2=1-p_1$. In this way, choosing an action in an uncertain environment can be interpreted as selecting a lottery over possible consequences, each defined by the combination of an action and the state of the world that materializes. This framework enables us to evaluate and compare actions by considering the expected value of their outcomes.

Consider the example in @tbl-expected-util, which we will call **payoff-matrix**, where we combine the actions with the states of the world and assign them utility values. As we can see, the action â€œsummer clothesâ€ might result in the consequence $x_{S,S}$ (summer clothes + sunny weather) with probability $p$, or $x_{S,R}$ (summer clothes + rainy weather) with probability $1 - p$. Since the actor cannot know for sure which state of the world will occur, they face a lottery: a **random** outcome yielding utility 10 in sunny weather or 4 in rainy weather.

|  |  |  |
|--------------------------------|--------------------|--------------------|
| **actions \\ states of the world** | **sunny weather** $(p)$ | **rainy weather** $(1-p)$ |
| **summer clothes** | $u(x_{S,S})=10$ | $u(x_{S,R})=4$ |
| **winter clothes** | $u(x_{W,S})=2$ | $u(x_{S,S})=3$ |
| **rain clothes** | $u(x_{S,S})=5$ | $u(x_{S,S})=8$ |

: Payoff matrix for a fictitious social scenario. {#tbl-expected-util}

In order to evaluate now how good this action is we must calculate the expected utility. Formally, suppose an actor faces a set of possible **states of the world** ($\omega \in \Omega$ ), each occurring with probability $p(\omega)$. The **expected utility** $EU$ of choosing $a$ is then:

$$ \mathrm{EU}(a) = \sum_{\omega \in \Omega} p(\omega) \cdot u\bigl(x_{a,\omega}\bigr)$$

::: callout-caution
As noted earlier, **expected utility** relies on **cardinal preferences**, meaning that the four key axioms of rational choice (completeness, transitivity, continuity, and independence) must be satisfied. Under these assumptions, we can define a **von Neumannâ€“Morgenstern (VNM) utility function**, which numerically represents an actorâ€™s preferences in a way that preserves both their ranking and the intensity of those preferences.
:::

This framework provides a **straightforward method** to compare actions under uncertainty. Because each action can be viewed as a **lottery** (a probabilistic combination of outcomes), we say an actor **prefers** action $a$ over action $b$ if and only if:

$$
a\succsim b \Leftrightarrow EU(a)\geq EU(b).
$$.

::: callout-note
-   While **utility** describes an actorâ€™s valuation of a single **specific consequence**, **expected utility** extends that concept to an entire **lottery** (i.e., a scenario with multiple possible outcomes and corresponding probabilities). By weighting each potential consequence according to its probability, expected utility provides a way to compare and rank **actions**â€”not just individual outcomesâ€”under uncertainty.
:::

### Beliefs

Finally, **beliefs** refer to the **probability distribution** (,p,) that an actor assigns to each possible **state of the world**. We introduced this concept implicitly in the Expected Utility section. By definition, each probability $p(\omega)$ must be at least zero $(p(\omega) \ge 0)$ , and the sum of all probabilities across every possible state must equal one. Thatâ€™s typically all we need to assume about beliefs at this stage.

Note, that we always assume that actors are capable of assigning probabilities to states of the worldâ€”even if those probabilities are subjective. At first glance, this may seem unrealistic. However, people often have intuitive or "gut" feelings about how likely different outcomes are. In fact, thereâ€™s an entire literature on how to measure and formalize such intuitions as **subjective probabilities** [e.g., @savage1954].

That said, we wonâ€™t delve deeper into the philosophical or psychological foundations of belief formation here. Instead, we will simply **assume** that actors assign probabilities to all relevant states and proceed on that basis.

## Analyzing

To analyze how an imaginary actor would behave in a given situation, we introduce a **choice function**, denoted by $C(\cdot )$. This function expresses our prediction of which action(s) an actor will choose from a given decision problem $D$, where $D\subset A$ represents the set of feasible actions available in that situation.

### Decisions under certainty

When the situation involves **certainty**â€”that is, there is only one possible state of the worldâ€”the actor knows exactly what consequence each action will lead to. In this case, we assume the actor simply chooses the action that results in the highest utility:\
$$ C^\succsim(D) \;=\; \arg\max_{a \in D} \mathrm{EU}(a)=\arg\max_{a \in D} \mathrm{u}(x_{a,\omega}). $$

Since there is no uncertainty, the best action is the one yielding the highest known outcome. For example, if we assume in @tbl-expected-util that $p=1$, that is, sunny weather is guaranteed, then the consequence of each action is known with certainty. Under these conditions, we can immediately predict that the actor would choose **summer clothes**, as they yield the highest utility (10) in sunny weather.

::: callout-note
-   In situations of certainty, we predict that the **actor** will **choose** the **action that maximizes** their **utility**.
:::

### Decisions under uncertainty

When the actor faces **uncertainty**â€”meaning that multiple states of the world are possible, each with its own probability â€”we use the concept of **expected utility**.

The **Expected Utility Hypothesis** posits that the actor will choose the action that **maximizes expected utility**. Formally:

$$ C^\succsim(D) \;=\; \arg\max_{a \in D} \mathrm{EU}(a). $$

Let us assume in @tbl-expected-util that $p=0.5$, so both sunny and rainy weather are equally likely. We then calculate the expected utility for each action:

-   $EU(\text{summer clothes}) = 0.5 \cdot 10 + 0.5 \cdot 4 = 7$,
-   $EU(\text{winter clothes}) = 0.5 \cdot 2 + 0.5 \cdot 3 = 2.5$, and
-   $EU(\text{rain clothes}) = 0.5 \cdot 5 + 0.5 \cdot 8 = 6.5$.

In this case, the actor would choose summer clothes, since that action yields the highest expected utility.

Note that, the use of numerical values to represent preferences sometimes draws criticism. Critics argue that we cannot realistically measure (nor can an outside observer precisely know) whether someone values one outcome at, say, 4 versus 3. This appears to impose a level of detail that is often unattainable in real-world studies. That critique is not unwarranted. However, as we will see later, in the vast majority of cases, precise numerical assessments of preferences are unnecessary. Even a rough ordinal understanding (e.g., which outcome is preferred over which) is sufficient for deriving most hypotheses and making meaningful predictions in practice.

For example, we can analyze how changing the probability $p$ affects the actorâ€™s decision. If $p$ increases, making sunny weather more likely, the expected utility of wearing **summer clothes** rises. Conversely, if decreases, making rainy weather more probable, the expected utility of wearing **rain clothes** becomes more attractive.

In this way, even simple models can offer powerful insights into how beliefs and preferences shape decision-making.

**Full Example**

Letâ€™s put everything together in an applied example. Imagine someone deciding whether or not to buy a tram ticket. The first thing we have to do is to ask ourselfs: What are the possible **actions**, **states of the worlds**?

-   **Actions**:
    -   Buy ticket $a_t$,
    -   Donâ€™t buy ticket $a_{nt}$.
-   **States of the world**:
    -   Ticket inspection $\omega_i$, with probability $p$,
    -   No ticket inspection $\omega_{ni}$, with probability $1-p$.

We can now bring all the elements together into a structured **payoff matrix** to illustrate how expected utility applies in a real-world setting. Rather than assigning fixed numerical values, we define variables that represent key components of the situation. In @tbl-decision-ticket, we introduce a ticket price $T$, a fine $P$, and a status quo $Q$.

To evaluate preferences, we assume a plausible ordinal ranking: $Q>-T>-F$. This suggests the actor prefers not paying and not getting caught ($Q$) over paying for the ticket ($-T$), but still much more than being fined ($-F$) .

That said, this ranking is not set in stone. As a researcher, you may adopt alternative assumptions. For example, if riding without a ticket causes guilt, the outcome might instead be represented as $Q-G$, where $G$ reflects a emotional cost (guild) that reduces the utility of this outcome so that $-T>Q-G>-P$ holds, in which case buying a ticket is preferred in every scenario.

In such a setup, we would say that action $a_t$ **dominates** action $a_{nt}$, because it performs better regardless of the state of the world. A dominated action is **never** chosen by a rational actor. Thus, our prediction becomes simple: the actor will always choose to buy a ticket.

|  |  |  |
|--------------------------------|--------------------|--------------------|
| **actions \\ states of the world** | **Ticket inspection (**$p$) | **No ticket inspection** ($1-p$) |
| **Buy ticket** ($a_t$) | $-T$ | $-T$ |
| **Donâ€™t buy ticket** ($a_{nt}$) | $-F$ | $Q$ |

: Payoff matrix for the ticket decision. {#tbl-decision-ticket}

Let us come now back to our original assumption $Q>-T>-F$. Please make sure that in this case neither of the two strategies dominates the other, as in one case $a_t$ is better, in the other $a_{nt}$. We now want to determine whether the actor prefers buying a ticket over not buying a ticket, i.e., whether the preference relation\
$$ a_t \succ a_{nt} $$\
holds.

To evaluate this, we use **expected utility**, which allows us to translate preference questions into comparisons of expected utilities. That is, the actor prefers buying a ticket if and only if:

$$ \mathrm{EU}(a_t) > \mathrm{EU}(a_{nt}). $$

This is often the central question we want to answer as researchers: under which conditions does an actor choose one action over another? So, now we can use the information from @tbl-decision-ticket, to replace these terms with:

$$ p \cdot (-T) + (1-p) \cdot (-T) > p \cdot (-F) + (1-p) \cdot Q.$$

The left side can easy reduced to:

$$ -T > p \cdot (-F) + (1-p) \cdot Q.$$

To make us the life somewhat easier lets use the tip (see box) and replace Q with 0.

$$ -T > p \cdot (-F).$$ {#eq-dec-ticket}

::: callout-tip
VNM utility values are measured on an interval scale, not a ratio scale. This means they lack an absolute zero point. We can exploit this property to our advantage. Specifically, without loss of generality, we can freely assign any two values to two entries in our payoff matrixâ€”as long as the ordinal ranking is preserved. Although itâ€™s not always necessary to redefine two variables, it is almost always useful to fix at least one variable to zero, particularly if that variable is not of central interest. This simplification can make analysis more convenient without affecting the underlying preferences.
:::

With inequality @eq-dec-ticket, we can now derive several testable hypotheses about behavior:

1.  **Effect of ticket price (**$T$):\
    If the price of the ticket $T$ increases, the left-hand side becomes **more negative**, making the inequality **less likely** to hold.\
    âŸ¶ The actor becomes **less likely** to buy a ticket.

2.  **Effect of fine (**$F$):\
    If the fine $F$ increases, the right-hand side becomes **more negative** (since $-F$ becomes a larger negative number), making the inequality **more likely** to hold.\
    âŸ¶ The actor becomes **more likely** to buy a ticket.

3.  **Effect of inspection probability (**$p$):\
    If the probability of inspection $p$ increases, the right-hand side puts **more weight** on the fine and **less on the benefit of avoiding payment**. As a result, the expected cost of not buying a ticket **increases**.\
    âŸ¶ The actor becomes **more likely** to buy a ticket.

Importantly, as researchers, we **do not need to know the exact numerical values** of $T$, $F$, or $Q$ to make meaningful predictions. As long as we have a reasonable **ordinal ranking**â€”such as $Q > -T > -F$â€”we can still analyze how changes in the environment (e.g., an increase in the fine $F$, a change in inspection probability $p$, or the introduction of guilt $G$) would influence decision-making. This kind of analysis can yield rich insights even when only rough preference orderings are available.

::: callout-note
-   Decision-making under **uncertainty** is about choosing the action with the highest **expected utility**â€”a weighted average of possible outcomes, according to their probabilities.
:::

# Game Theory

**Game theory** is the study of strategic situations where the outcome of a decision depends not just on a single actorâ€™s choice, but on the **interdependent actions of multiple actors**. In contrast to the models weâ€™ve discussed so farâ€”where the environment was treated as fixed or probabilisticâ€”game theory explicitly models the fact that **other people matter**: their decisions affect our outcomes, and vice versa.

Luckily, much of the groundwork for studying games is already in place. We do not need to introduce entirely new concepts. We still rely on familiar elements: actions, outcomes, and preferences. What changes is how we think about uncertainty. In game theory, the uncertainty often comes not from possible states of the nature (as in previous sections), but from the fact that we do **not know what others will do**. However, just like before, we can reason through expected utilitiesâ€”only now, those expectations depend on the beliefs about other **players**.

## Modelling

### Players

In game theory, we refer to the actors as **players**. This terminology immediately signals that we are analyzing a situation through a game-theoretic lens. We define the set of all players as a **finite set** $N$. While in principle we could label the players with namesâ€”such as $N = \{\text{Ann}, \text{Bob}, \text{Charlie}, \dots\}$â€”it is standard practice to **number** them instead. So we typically write:

$$
N = \{1, 2, 3, \dots, n\},
$$

where each number represents a different player. This not only simplifies notation but also generalizes easily to games with any number of participants. Each player $i \in N$ has a set of available **actions** $A_i$, and a **Von Neumann-Morgenstern utility function** $u_i$ that assigns a utility value to every possible outcome. We denote the set of all players except player $i$ as $-i$, that is, $-i=N\{i\}$.

The set of **outcomes** in a game can be represented as all possible combinations of players' actions (sometimes called **action profiles**) and the states of the world $\Omega$. That is, we define:

$$
X = A_1 \times A_2 \times \cdots \times A_n \times \Omega,
$$

However, to keep things simple, we will mostly focus on situations where $|\Omega| = 1$, meaning there is no external uncertainty. This allows us to **concentrate solely on strategic interaction**, i.e., how playersâ€™ choices affect one another. This simplification is also common in many textbooks, so donâ€™t be surprised if you come across the outcome space written as:

$$
X = A_1 \times A_2 \times \cdots \times A_n.
$$

An intuitive way to think about outcomes from the perspective of one player is this: the actions of all *other* players form the "state of the world" from their point of view. That is, even though those actions are not random per se, the player cannot control them and must form expectations about themâ€”just like in earlier decision problems.

::: callout-note
-   In a game actors are called **players**, and we use a finite set $N = {1, 2, \dots, n}$ to represent them.
-   Each player $i$ has its own set of actions $A_i$ and utility function $u_i$.
-   The full outcome space is based on **combinations of all playersâ€™ actions**, possibly including a state of the world $\Omega$.
:::

### A Simple Example

To illustrate the basic structure of a game, letâ€™s consider a classic **2-player game** where each player has two strategies and the state of the world is certain. We represent this situation in a **payoff matrix**. Each cell of the table shows the utility (or payoff) for both players, given their chosen actions.

|                         | Player 2: Cooperate | Player 2: Defect |
|-------------------------|---------------------|------------------|
| **Player 1: Cooperate** | (3, 3)              | (0, 5)           |
| **Player 1: Defect**    | (5, 0)              | (1, 1)           |

: A simple 2-player game with two strategies each (e.g., the Prisonerâ€™s Dilemma). The first number in each cell is Player 1â€™s utility; the second is Player 2â€™s. {#tbl-game-pd}

In this example, both players have the same two actions: **Cooperate** and **Defect**. The outcome depends on the combination of both players' choices. For instance, if both cooperate, they each get 3.[^4] If one defects while the other cooperates, the defector gets 5 while the cooperator gets nothing. This structure introduces **strategic uncertainty**: each player must consider not just what they want, but what the other is likely to do.

[^4]: For the sake of simplicity, we say "a player gets 3" instead of "a player receives a utility of 3 from this outcome". Please note that this doesnâ€™t mean the player literally receives money or any tangible rewardâ€”it's just a representation of their preference or satisfaction with the outcome.

## Analyzing

Now that we have a basic understanding of what a game looks like, the next step is to analyze it. The central solution concept in game theory is the **Nash equilibrium**.

A **Nash equilibrium** is a set of strategiesâ€”one for each playerâ€”such that **no player has an incentive to unilaterally deviate**. In other words, given the strategies chosen by the others, each player is doing the best they can. Formally, a **action profiles** $a^*=(a_1^*, a_2^*, \dots, a_n^*)$ is a pure Nash equilibrium if for every player $i$, if it holds that:[^5]

[^5]: We will learn about the differences between a pure and a mixed Nash equilibrium in the next section.

$$
u_i(a_i^*, a_{-i}^*) \ge u_i(a_i, a_{-i}^*) \quad \text{for all } a_i \in A_i.
$$

This means: given what everyone else is doing ($a_{-i}^*$), player $i$ cannot improve their utility by choosing a different action $a_i$.

Letâ€™s go back to our example from @tbl-game-pd and analyze it.

To find the Nash equilibrium:

1.  **Fix Player 2â€™s strategy**, and ask what Player 1 would do:
    -   If Player 2 **cooperates**, Player 1 gets 3 by cooperating and 5 by defecting â†’ **Player 1 prefers to defect**.
    -   If Player 2 **defects**, Player 1 gets 0 by cooperating and 1 by defecting â†’ **Player 1 still prefers to defect**.
2.  **Fix Player 1â€™s strategy**, and ask what Player 2 would do:
    -   If Player 1 **cooperates**, Player 2 gets 3 by cooperating and 5 by defecting â†’ **Player 2 prefers to defect**.
    -   If Player 1 **defects**, Player 2 gets 0 by cooperating and 1 by defecting â†’ **Player 2 still prefers to defect**.

So, both players have a **dominant strategy**: defecting. Regardless of what the other does, defecting gives a higher payoff. The strategy profile where **both defect** is therefore a **Nash equilibrium**, formally: $a^*=(a_D,a_D)$ with the first $a_D$ is the action of the first player, and so on.

Even though both players would be better off if they both cooperated (3, 3), bilateral cooperation is **not** stable: each has an incentive to unilaterally deviate if the other is cooperating. This tension between individual rationality and collective outcome is what makes game theory so powerfulâ€”and often so frustrating.

A game can easily have **more than one Nash equilibrium**, as illustrated by the game in @tbl-game-sd. This game models a situation where two countries must independently decide whether to **arm themselves** or **refrain** from doing so. In this example, there are **two pure strategy Nash equilibria**. Each countryâ€™s best decision depends on what the other does, and both (Arm, Arm) and (Refrain, Refrain) are stable outcomes where no player has an incentive to deviate unilaterally.

|                       | Player 2: Refrain | Player 2: Arm |
|-----------------------|-------------------|---------------|
| **Player 1: Refrain** | (3, 3)            | (0, 2)        |
| **Player 1: Arm**     | (2, 0)            | (1, 1)        |

: A coordination game with two pure Nash equilibria. {#tbl-game-sd}

### Mixed Nash Equilibrium

So far, we have focused on **pure strategies**, where each player selects a single action with certainty. However, there are many gamesâ€”especially competitive onesâ€”where no pure Nash equilibrium exists. In such cases, we extend our analysis to include **mixed strategies**.

#### Mixed Strategies

A **mixed strategy** is a probability distribution over a playerâ€™s set of available actions. Instead of choosing a single action, the player **randomizes** across multiple actions, assigning a probability to each.

Let $A_i = \{a_{i1}, a_{i2}, \dots, a_{ik}\}$ be the action set for player $i$. A **mixed strategy** $\sigma_i$ is a vector of probabilities:

$$
\sigma_i = \left( p_{i1}, p_{i2}, \dots, p_{ik} \right),
$$

where each $p_{ij} \in [0, 1]$ and $\sum_{j=1}^k p_{ij} = 1$.

The set of all possible mixed strategies for player $i$ is denoted by $\Delta(A_i)$â€”the **simplex** over $A_i$. This set includes all possible ways a player could randomize over their actions, including the pure strategies (which are just special cases where one action is chosen with probability 1).

Choosing a strategy rather than an action is quite common. Just think about playing **rock-paper-scissors**: if you always choose the same move, your opponent can easily exploit you. But if you randomizeâ€”say, by playing each option with equal probabilityâ€”you become unpredictable and much harder to beat. This is exactly the idea of a mixed strategy: your best response might not be to pick a single action, but rather to **mix** between several.[^6]

[^6]: There is an ongoing philosophical debate about whether humans are truly capable of generating random behavior. Empirical studies suggest that humans often struggle to produce genuinely random sequences, tending instead toward predictable patterns [@wagenaar1972, @nickerson2009]. However, in strategic contexts, perfect randomness may not be necessary. What matters more is that an individual's behavior is sufficiently unpredictable to others, effectively mimicking randomness. This perceived unpredictability can be strategically advantageous, as it prevents opponents from exploiting discernible patterns. While this topic delves into complex philosophical and psychological discussions, for our purposes, we will assume that players can adopt mixed strategies that achieve the necessary level of unpredictability.

Once all players are allowed to mix, the outcome of the game becomes **probabilistic**. The expected utility of a player under a mixed strategy profile is calculated by taking the weighted average of their utility across all possible action profiles, weighted by the probability that each occurs under the playersâ€™ strategies.

This framework allows us to meaningfully analyze games that have **no pure strategy Nash equilibrium**, by expanding the space of strategies to include randomization.

::: callout-note
-   A **strategy** $\sigma$ is a **probability distribution** over the set of actions.
:::

#### **Mixed Nash Equilibrium**

A **mixed strategy Nash equilibrium** is a profile of mixed strategiesâ€”one for each playerâ€”such that no player can increase their expected utility by unilaterally changing their own strategy. Each player is playing a best response to the **distribution** of strategies chosen by others.

Formally, let $\Delta(A_i)$ denote the set of probability distributions over player $i$'s action set $A_i$. Then a mixed strategy profile $\sigma^* = (\sigma_1^*, \dots, \sigma_n^*)$ is a Nash equilibrium if, for each player $i$,

$$
\mathrm{EU_i}(\sigma^*, \sigma_{-i}^*) \geq \mathrm{EU_i}(\sigma_i, \sigma_{-i}^*) \quad \text{for all } \sigma_i \in \Delta(A_i).
$$

This means that given the other players' mixed strategies, player $i$ has no incentive to unilaterally change their own strategy.

### Example: Matching Pennies

Letâ€™s consider a simple 2-player game called **Matching Pennies**. Each player chooses Heads (H) or Tails (T). If the pennies match, Player 1 wins; if they donâ€™t, Player 2 wins.

|                           | Player 2: H ($p_2$) | Player 2: T ($1-p_2$) |
|---------------------------|---------------------|-----------------------|
| **Player 1: H** ($p_1$)   | (1, -1)             | (-1, 1)               |
| **Player 1: T** ($1-p_1$) | (-1, 1)             | (1, -1)               |

: Matching Pennies. The first number in each cell is Player 1â€™s utility; the second is Player 2â€™s. {#tbl-game-mp}

There is **no pure strategy Nash equilibrium** in this game: whatever one player does, the other has an incentive to switch. But there is a **mixed strategy equilibrium**.

::: callout-important
One of the most fundamental results in game theory is **Nashâ€™s Theorem**. It tells us something very reassuring: **every finite game has at least one Nash equilibrium** (either in pure or mixed strategies). A **finite game** is simply a game where there is a **finite number of players**, and each player has a **finite number of actions** to choose from.

Nashâ€™s Theorem guarantees that, even if players must randomize over their actions, there will always be at least one strategy profile where no player has an incentive to deviate. This means that game theory, even in its most basic form, always provides a solution concept that predicts stable behavior.
:::

To find the mixed strategy equilibrium, we use the same logic as in decisions under uncertainty. But this time, the **"uncertainty" comes from the other player's strategy**, and we want to find a situation where each player is **indifferent** between their available actionsâ€”meaning they have no incentive to shift their probabilities. Hence, we look for values of $p_1$ and $p_2$ where the following equations hold true:

$$
EU_1(a_{1H})=EU_1(a_{1T}),
$$

$$ EU_2(a_{2H})=EU_2(a_{2T}).$$

Letâ€™s compute Player 1â€™s expected utility from playing Heads or Tails, given Player 2 plays Heads with probability $p_2$ and Tails with probability $1 - p_2$:

-   Expected utility of **H** for Player 1:\
    $$
    EU_1(a_{1H}) = p_2 \cdot 1 + (1 - p_2) \cdot (-1) = 2p_2 - 1
    $$

-   Expected utility of **T** for Player 1:\
    $$
    EU_1(a_{1T}) = p_2 \cdot (-1) + (1 - p_2) \cdot 1 = 1 - 2p_2
    $$

Player 1 is **indifferent** between $a_{1H}$ and $a_{1T}$ when:

$$
EU_1(a_{1H}) = EU_1(a_{1T}) \quad \Rightarrow \quad 2p_2 - 1 = 1 - 2p_2
$$

Solving gives:

$$
4p_2 = 2 \quad \Rightarrow \quad p_2 = 0.5
$$

So Player 1 is indifferent **only if** Player 2 randomizes 50/50 between $a_{2H}$ and $a_{2T}$.

::: callout-important
Note that we determined Player 2â€™s behavioral predictions **even though we started by calculating the expected utilities for Player 1**. This might seem puzzling at firstâ€”after all, why should analyzing one player's incentives tell us what the other player will do?

But this makes sense when you think in terms of **strategic interaction**. The key idea in a mixed strategy Nash equilibrium is that each player is choosing their strategy so that the **other player is indifferent** between their own actions. So by solving for the condition under which Player 1 is indifferent, weâ€™re actually **learning what Player 2 must be doing** to create that indifferenceâ€”and vice versa.

In this way, even though each player is focused on their own payoffs, their optimal behavior is tightly linked to the incentives of the other. Thatâ€™s the essence of equilibrium thinking in game theory.
:::

We repeat the same for Player 2:

-   Expected utility of $a_{2H}$ for Player 2:\
    $$
    EU_2(a\_{2H}) = p_1 \cdot (-1) + (1 - p_1) \cdot 1 = 1 - 2p_1
    $$

-   Expected utility of $a_{2T}$ for Player 2:\
    $$
    EU_2(a\_{2T}) = p_1 \cdot 1 + (1 - p_1) \cdot (-1) = 2p_1 - 1
    $$

Indifference occurs when:

$$
1 - 2p_1 = 2p_1 - 1 \quad \Rightarrow \quad 4p_1 = 2 \quad \Rightarrow \quad p_1 = 0.5
$$

So Player 2 is also indifferent only if Player 1 plays H and T with equal probability.

The unique **mixed strategy Nash equilibrium** of Matching Pennies is:

$$
s^*=((0.5,H;0.5;T), (0.5,H;0.5;T))
$$

In this equilibrium, each player keeps the other perfectly indifferent and maximally uncertainâ€”neither can benefit by changing their strategy.

# ðŸ‹ï¸ **Pre-Class Assignment**

1.  Read the assigned materials.

2.  \[easy\] Look at @tbl-expected-util. Calculate the the expected utilities for each action if $p=0.25$.

3.  \[easy\] Model a socially relevant decision situation, including important variables.

4.  \[intermediate\] Look at the game shown in @tbl-game-sd. This game also has a third equilibrium in mixed strategies; determine it.

5.  Remember Boudon's [-@boudon1977] model of relative deprivation? It considers only three factors: the costs $c$ of applying for a promotion, the benefits $b$ of receiving the promotion, and the relative share of available positions $f$, all of which are assumed to be common knowledge.

    1.  \[easy\] Model this situation with $n=2$ and $f=.5$.

    2.  \[hard\] Try to derive the general function of p for any $n$ and $f$.

6.  Bring your results and any questions to the seminar.
